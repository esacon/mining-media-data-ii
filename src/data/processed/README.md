# Processed Data Directory

This directory contains all processed data files generated by the data preparation and dataset creation pipeline.

## File Structure

### JSONL Files (Player Events)
- `game1_player_events.jsonl` - Game 1 data converted from CSV to JSONL format
- `game1_player_events_train.jsonl` - 80% training split of Game 1 players
- `game1_player_events_eval.jsonl` - 20% evaluation split of Game 1 players
- `playerLogs_game2_playerbasedlines_train.jsonl` - 80% training split of Game 2 players
- `playerLogs_game2_playerbasedlines_eval.jsonl` - 20% evaluation split of Game 2 players

### Labeled Datasets
- `game1_DS1_labeled.jsonl` - Game 1 training dataset with churn labels
- `game1_DS2_labeled.jsonl` - Game 1 evaluation dataset with churn labels
- `game2_DS1_labeled.jsonl` - Game 2 training dataset with churn labels
- `game2_DS2_labeled.jsonl` - Game 2 evaluation dataset with churn labels

## Data Formats

### Player Events JSONL Format
Each line contains a JSON object with:
```json
{
  "device_id": "string",  // or "uid" for Game 2
  "records": [
    {
      "time": integer,      // Unix timestamp (seconds for Game 1, milliseconds for Game 2)
      "event": "string",    // Event type (e.g., "play", "softPurchase")
      "score": integer,     // Score (for play events)
      // Additional fields for Game 2 events
    }
  ]
}
```

### Labeled Dataset Format
Each line contains a JSON object with:
```json
{
  "player_id": "string",
  "observation_records": [...],      // Events in 5-day observation period
  "churn_period_records": [...],     // Events in 10-day churn period
  "churned": boolean,                // True if no events in churn period
  "op_start": "ISO datetime",        // Observation period start
  "op_end": "ISO datetime",          // Observation period end
  "cp_start": "ISO datetime",        // Churn period start
  "cp_end": "ISO datetime",          // Churn period end
  "op_event_count": integer,         // Number of events in observation period
  "cp_event_count": integer          // Number of events in churn period
}
```

## Feature Engineering

### Original Kim et al. (2017) Features

The feature extraction process generates CSV files in the `results/features/` directory following the original Kim et al. (2017) methodology:

### Common Features (Both Games)

**10 common features** extracted from play patterns and scores:

#### **Play Pattern Features**
- **playCount**: Total number of plays in observation period
- **activeDuration**: Time difference between last and first play in observation period
- **consecutivePlayRatio**: Ratio of consecutive plays (where time between plays < threshold)

#### **Score-Related Features**
- **bestScore**: Maximum score achieved during observation period
- **meanScore**: Average score during observation period
- **worstScore**: Minimum score achieved during observation period
- **sdScore**: Standard deviation of scores in observation period
- **bestScoreIndex**: Index of best score normalized by play count
- **bestSubMeanCount**: Difference between best and mean score, normalized by play count
- **bestSubMeanRatio**: Ratio between (best score - mean score) and mean score

### Game-Specific Features

#### **Game 2 Only (Racing Game with Purchases)**
- **purchaseCount**: Total number of vehicle purchases in observation period
- **highestPrice**: Highest price among vehicle purchases in observation period

### Target Variable
- **churned**: Target variable (0 = retained, 1 = churned)

### Feature Files
- `game1_DS1_features.csv` - Extracted features for Game 1 training dataset
- `game1_DS2_features.csv` - Extracted features for Game 1 evaluation dataset
- `game2_DS1_features.csv` - Extracted features for Game 2 training dataset
- `game2_DS2_features.csv` - Extracted features for Game 2 evaluation dataset

### Key Findings from Kim et al. (2017)
- **Most Important Features**: `activeDuration` and `playCount` (play-time metrics)
- **Optimal Feature Count**: Only 2-3 features needed for effective churn prediction
- **Performance**: Adding more features beyond 2-3 provides minimal improvement

## Processing Pipeline

1. **Data Preparation** (`src/data_processing/data_preparation.py`)
2. **Dataset Creation** (`src/data_processing/dataset_creation.py`)
3. **Feature Engineering** (`src/data_processing/feature_engineering.py`)
