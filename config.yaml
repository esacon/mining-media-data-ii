# Churn Prediction Project Configuration

# Data Processing Parameters
data_processing:
  observation_days: 5          # Days for observation period
  churn_period_days: 10        # Days for churn prediction period
  train_ratio: 0.8             # Train/eval split ratio
  random_seed: 42              # Random seed for reproducibility

# Directory Paths (relative to project root)
paths:
  data_dir: "src/data"
  processed_dir: "src/data/processed"
  logs_dir: "logs"
  results_dir: "results"
  features_dir: "results/features"

# File Names Configuration
filenames:
  # Input files
  game1_csv: "dataset_game1/rawdata_game1.csv"
  game2_jsonl: "dataset_game2/playerLogs_game2_playerbasedlines.jsonl"

  # Intermediate files (after conversion and splitting)
  game1_converted: "game1_player_events.jsonl"
  game1_train: "game1_player_events_train.jsonl"
  game1_eval: "game1_player_events_eval.jsonl"
  game2_train: "playerLogs_game2_playerbasedlines_train.jsonl"
  game2_eval: "playerLogs_game2_playerbasedlines_eval.jsonl"

  # Final labeled datasets
  game1_ds1: "game1_DS1_labeled.jsonl"
  game1_ds2: "game1_DS2_labeled.jsonl"
  game2_ds1: "game2_DS1_labeled.jsonl"
  game2_ds2: "game2_DS2_labeled.jsonl"

  # Feature files
  game1_ds1_features: "game1_DS1_features.csv"
  game1_ds2_features: "game1_DS2_features.csv"
  game2_ds1_features: "game2_DS1_features.csv"
  game2_ds2_features: "game2_DS2_features.csv"

  # File suffixes
  train_suffix: "_train.jsonl"
  eval_suffix: "_eval.jsonl"
  labeled_suffix: "_labeled.jsonl"
  features_suffix: "_features.csv"

# Feature Engineering Configuration
feature_engineering:
  # Common features (from Kim et al. 2017)
  common_features:
    - "playCount"              # Total number of plays in observation period
    - "bestScore"              # Maximum score achieved
    - "meanScore"              # Average score
    - "worstScore"             # Minimum score achieved
    - "sdScore"                # Standard deviation of scores
    - "bestScoreIndex"         # Index of best score (normalized)
    - "bestSubMeanCount"       # (Best - Mean) / Play count
    - "bestSubMeanRatio"       # (Best - Mean) / Mean
    - "activeDuration"         # Time between first and last play (hours)
    - "consecutivePlayRatio"   # Ratio of consecutive plays

  # Game 2 specific features (Kim et al. 2017)
  game2_specific_features:
    - "purchaseCount"          # Total number of vehicle purchases
    - "highestPrice"           # Highest price among purchases

# Model Configuration
models:
  # Data preprocessing
  multicollinearity_threshold: 0.95  # Correlation threshold for feature removal

  # Model parameters
  decision_tree:
    random_state: 42
    max_depth: 10                    # Constrain depth to prevent overfitting
    min_samples_split: 20
    min_samples_leaf: 10
    criterion: "gini"
    max_features: "sqrt"             # Use feature selection
    class_weight: "balanced"         # Handle class imbalance
    ccp_alpha: 0.01                  # Cost complexity pruning

  logistic_regression:
    random_state: 42
    solver: "saga"               # Good for large datasets and L1/L2 penalties
    penalty: "l2"                # L2 regularization
    max_iter: 5000               # Increased for convergence
    C: 0.1                       # Regularization strength
    tol: 0.001                   # Tolerance for stopping criterion
    class_weight: "balanced"     # Handle class imbalance

  random_forest:
    random_state: 42
    n_estimators: 100            # Number of trees
    max_depth: 15
    min_samples_split: 10
    min_samples_leaf: 5
    max_features: "sqrt"         # Features to consider at each split
    n_jobs: -1                   # Use all available cores
    bootstrap: true
    oob_score: true              # Out-of-bag scoring
    class_weight: "balanced"     # Handle class imbalance

# Logging Configuration
logging:
  level: "INFO"                # DEBUG, INFO, WARNING, ERROR, CRITICAL
  console: true                # Log to console
  file: true                   # Log to file

# Performance Settings
performance:
  batch_size: 1000             # Batch size for processing
  progress_interval: 1000      # Progress logging interval
